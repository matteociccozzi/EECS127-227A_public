{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Regression Playground\n",
    "\n",
    "EECS 127/227a\n",
    "\n",
    "In this notebook, we will explore 4 different types of regression: Ordinary Least Squares, Ridge Regression, Weighted Least Squares, and Tikhonov Regularization. Only modify code in the sections marked TODO and fill in your answers to the questions marked TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib.font_manager import FontProperties\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "np.set_printoptions(3, suppress=True)\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fn(X):\n",
    "\n",
    "    y = 1/1000*(X - 6)*(X - 4)*(X - 2)*(X + 1)*(X + 3)*(X + 5)*(X + 7)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(m, n, poly_degree=1, generate_bad_data=False):\n",
    "\n",
    "    X = np.linspace(-7, 6, m).reshape(-1, 1)\n",
    "    \n",
    "    y = data_fn(X)\n",
    "    \n",
    "    noise = np.random.normal(0, 2, size=X.shape[0]).reshape(-1, 1)\n",
    "    y_obs = y + noise\n",
    "    \n",
    "    bad_noise = np.random.normal(0, 10, size=X.shape[0]).reshape(-1, 1)\n",
    "    y_obs_bad = y + bad_noise\n",
    "    \n",
    "    # create polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_degree)\n",
    "    X_hat = poly.fit_transform(X)\n",
    "\n",
    "    X_true = np.linspace(-7.1, 6.1, 1000).reshape(-1, 1)\n",
    "    y_true = data_fn(X_true)\n",
    "    \n",
    "    X_hat_true = poly.fit_transform(X_true)\n",
    "    \n",
    "    # Note we are creating \"test\" data here, but this is really like a true error, since we have the underlying polynomial at hand\n",
    "    # So even though we add some noise, this is a bit artificial\n",
    "    y_test = data_fn(X) + np.random.normal(0, 2, size=X.shape[0]).reshape(-1, 1)\n",
    "    \n",
    "    return X_hat, X, X_hat_true, X_true, y_obs, y_obs_bad, y_true, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(X_hat, X, X_true, y_obs, y_true, y_test, ws, y_obs_bad=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "    \n",
    "    ax[0].set_title(\"Regression\")\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[0].set_ylabel(\"y\")\n",
    "    ax[0].grid()\n",
    "    ax[0].scatter(X, y_obs, label=\"Observations\", alpha=0.5, color=\"green\", marker=\"+\")\n",
    "    ax[0].plot(X_true, y_true, label=\"Groud Truth\", alpha=0.5, color=\"orange\", linestyle=\"--\")\n",
    "    ax[0].scatter(X, y_test, label=\"Test Set\", color=\"blue\", alpha=0.5, marker=\"*\")\n",
    "    \n",
    "    if y_obs_bad is not None:\n",
    "        ax[0].scatter(X, y_obs_bad, label=\"Bad Observations\", alpha=0.5, color=\"red\", marker=\"x\")\n",
    "    \n",
    "    for w, name in ws:\n",
    "        ax[0].scatter(X, X_hat @ w, label=name)\n",
    "    ax[0].legend(prop=fontP)\n",
    "\n",
    "    ax[1].set_title(\"Visualizing w\")\n",
    "    ax[1].grid()\n",
    "    \n",
    "    for w, name in ws:\n",
    "        ax[1].scatter(w[0][0], w[1][0], label=name)\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Data Generation\n",
    "Our data will be generated from some pre-defined function, but with some Gaussian/random noise added to it. Our goal will be to fit a 7-degree polynomial to this data. We have two sets of observations. \"Good observations\" are those which were collected with a small amount of noise, but \"Bad observations\" were collected with much more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat, X, X_hat_true, X_true, y_obs, y_obs_bad, y_true, y_test = generate_data(30, 1, poly_degree=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data Generated\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(X_true, y_true, label=\"Ground Truth\", color=\"orange\")\n",
    "plt.scatter(X, y_obs, label=\"Good Observations\", color=\"green\", marker=\"+\")\n",
    "plt.scatter(X, y_obs_bad, label=\"Bad Observations\", color=\"red\", marker=\"x\")\n",
    "plt.scatter(X, y_test, label=\"Test Set\", color=\"blue\", marker=\"*\")\n",
    "plt.legend(prop=fontP)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ordinary Least Squares (OLS)\n",
    "We'll begin with the standard least-squares problem. Given some data in the form of $(x, y)$ pairs, we wish to find the polynomial that will minimize the sum of the squared errors (SSE):\n",
    "\n",
    "$$\\min \\sum_{i}(y_i - w^\\top x_i)^2 = \\min_{w} \\|Xw - y\\|_2$$\n",
    "\n",
    "Note that even though we are fitting a polynomial, the objective is still linear in the weights vector $w$. This is because we create   features in the data matrix corresponding to higher order polynomial terms:\n",
    "\n",
    "$$X = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m \\end{bmatrix} \\rightarrow \\hat{X} = \\begin{bmatrix} x_1 & x_1^2 & \\dots & x_1^d \\\\ x_2 & x_2^2 & \\dots & x_2^d \\\\ \\vdots \\\\ x_m & x_m^2 & \\dots & x_m^d \\end{bmatrix}$$\n",
    "\n",
    "So now, each $w_i$ tunes the effect of adding an $i$th order term. This is known as $\\textbf{lifting}$ the data into higher-dimensional space. Finding the \"line\" that regresses in this space is equivalent to finding the higher dimensional polynomial that actually fits the data.\n",
    "\n",
    "Disclaimer: For this example, we will ignore the bad observations. We will return to that problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a). TODO: Implement OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS(X, y, verbose=False):\n",
    "    \n",
    "    # TODO: Implement OLS \n",
    "    # Fill in the expressions for the pseudoinverse, X_dagger, and use that to compute the optimal w\n",
    "    \n",
    "    #----------\n",
    "    X_dagger = # complete this\n",
    "    w = # complete this to find optimal w \n",
    "    \n",
    "    #----------\n",
    "    error = np.linalg.norm(X @ w - y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"--------------------OLS-----------------------\\n\")\n",
    "        print(\"The optimal w is\", np.ndarray.flatten(w))\n",
    "        print(\"\\n\")\n",
    "        print(\"Training error:\", error)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return w, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, train_error_ls = OLS(X_hat, y_obs, verbose=True)\n",
    "test_error_ls = np.linalg.norm(X_hat @ w_ls - y_test)\n",
    "print(\"Testing error:\", test_error_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the 7 dimensional w is projected into two dimensions for plotting.\n",
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, [[w_ls, \"OLS\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge $(\\ell_2)$ Regression\n",
    "Vanilla least squares is great, but can be affected negatively by noise. Ordinary Least Squares (OLS) is susceptible to overfitting the data, since we only care about minimizing the squared error of the data we trained it on.\n",
    "\n",
    "We can attempt to prevent this overfitting by introducing a penalty term on the weight vector. If we penalize it from getting too large (numerically unstable), we can combat the impact of noise. The penalty term we will use here is the $\\ell_2$-norm:\n",
    "\n",
    "$$\\min_{w} \\|Xw - y\\|_2^2 + \\lambda\\|w\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.ii) TODO: Implement Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_lmbda = None\n",
    "ridge_lambdas = np.linspace(.0001,20, 10000)\n",
    "\n",
    "\n",
    "def ridge(X, y, lmbda, verbose=False):\n",
    "    \n",
    "    # TODO: Implement Ridge Regression\n",
    "    \n",
    "    #----------\n",
    "    \n",
    "    X_dagger = # Complete this\n",
    "    w = # Complete this to find optimal w\n",
    "    #----------\n",
    "    error = np.linalg.norm(X @ w - y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"--------------------Ridge-----------------------\\n\")\n",
    "        print(\"Lambda: \", lmbda)\n",
    "        print(\"\\n\")\n",
    "        print(\"The optimal w is\", np.ndarray.flatten(w))\n",
    "        print(\"\\n\")\n",
    "        print(\"Training error:\", error)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return w, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmater Tuning\n",
    "One thing to note is the $\\lambda$ in the problem. It is known as a $\\textbf{hyperparameter}$ and must be manually tuned, since we don't really know how much to penalize the norm. In order to find the best $\\lambda$ we test different values and see how well the learned model generalizes by computing the squared  error of the predictions of our model on unseen test data. Below, you can see the effect of different values of $\\lambda$ on the SSE for the training data, the SSE for the test data, and the components of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors_rr = []\n",
    "test_errors_rr = []\n",
    "ws_rr = []\n",
    "for l in ridge_lambdas:\n",
    "    w_rr, train_error_rr = ridge(X_hat, y_obs, l)\n",
    "    test_error_rr = np.linalg.norm(X_hat @ w_rr - y_test)\n",
    "    train_errors_rr.append(train_error_rr)\n",
    "    ws_rr.append(w_rr)\n",
    "    test_errors_rr.append(test_error_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.title(\"Error\")\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "#plt.xscale(\"log\")\n",
    "plt.plot(ridge_lambdas, train_errors_rr, label=\"Training Error\")\n",
    "plt.plot(ridge_lambdas, test_errors_rr, label=\"Test Error\")\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.title(\"Regularization Path of w\")\n",
    "num_coeffs = len(ws_rr[0])\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "#plt.xscale(\"log\")\n",
    "for i in range(num_coeffs):\n",
    "    plt.plot(ridge_lambdas, [w[i][0] for w in ws_rr], label=\"w_\" + str(i))\n",
    "plt.legend(prop=fontP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reflect on the above graphs: What do you observe in the graphs above? Why does increasing $\\lambda$ lead to these trends in the training error, testing error, and weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try changing these values, what do you see?\n",
    "test_lmbdas = [.1, 1, 2, 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rrs = []\n",
    "for l in test_lmbdas:\n",
    "    w_rr, train_error_rr = ridge(X_hat, y_obs, l, verbose=True)\n",
    "    test_error_rr = np.linalg.norm(X_hat @ w_rr - y_test)\n",
    "    print(\"Testing error:\", test_error_rr)\n",
    "    print(\"\\n\")\n",
    "    w_rrs.append([w_rr, r\"$\\lambda:$\" + str(l)])\n",
    "w_rrs.append([w_ls, \"OLS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_rrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Least Squares (WLS)\n",
    "Sometimes, our data observations $X$ may not be trustworthy. Perhaps multiple sources gathered data, but one is very noisy. How can we account for this? We can just weight each of the data points! Ideally, we would like data points that come from \"trustworthy\" sources to have a greater weight, so we can effectively perform a change of basis to a space that reflects this.\n",
    "$$\\min_{w} \\|W_1(Xw - y)\\|_2$$\n",
    "$$W_1 = diag(w_1, w_2, \\dots)$$\n",
    "\n",
    "To show the usefulness, we can compare how OLS would react to this extra \"bad\" data versus WLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append bad data to data matrix\n",
    "X_hat_with_bad = np.vstack((X_hat, X_hat))\n",
    "y_with_bad = np.vstack((y_obs, y_obs_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.i) TODO: Implement WLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(X, y, W_vec, verbose=False):\n",
    "    \n",
    "    W = np.diag(W_vec)\n",
    "    \n",
    "    # TODO: Implement WLS using the pseudoinverse\n",
    "    #----------\n",
    "    \n",
    "\n",
    "    X_dagger = # complete this\n",
    "    w = # complete this to find optimal w\n",
    "    \n",
    "    #----------\n",
    "    error = np.linalg.norm(X @ w - y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"--------------------WLS-----------------------\")\n",
    "        print(\"\\n\")\n",
    "        print(\"The optimal w is\", np.ndarray.flatten(w))\n",
    "        print(\"\\n\")\n",
    "        print(\"Training error:\", error)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return w, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose appropriate data weight vector (this will eventually become a diagonal matrix)\n",
    "# We only trust the first half of the data more than the second half\n",
    "\n",
    "wls_W1_vec = np.concatenate((np.ones(X.shape[0]), 0.5*np.ones(X.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_bad, train_error_ls_bad = OLS(X_hat_with_bad, y_with_bad, verbose=True)\n",
    "test_error_ls_bad = np.linalg.norm(X_hat @ w_ls_bad - y_test)\n",
    "print(\"Testing error:\", test_error_ls_bad)\n",
    "print(\"\\n\")\n",
    "\n",
    "w_wls, train_error_wls = WLS(X_hat_with_bad, y_with_bad, wls_W1_vec, verbose=True)\n",
    "test_error_wls = np.linalg.norm(X_hat @ w_wls - y_test)\n",
    "print(\"Testing error:\", test_error_wls)\n",
    "print(\"\\n\")\n",
    "\n",
    "w_wls = [[w_wls, \"WLS\"], [w_ls_bad, \"OLS w/ Bad Data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_wls, y_obs_bad=y_obs_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.ii) TODO: Comment on the difference between WLS and OLS w/ Bad Data, which one performed better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) WLS Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a very straightforward approach in how we treated the validity of the data. But it didn't turn out to work that well! The training error of OLS was actually pretty good compared to WLS, so perhaps stricly ignoring the data isn't the best choice. Now it's your turn to play around with different $W_1$ values. See what happens when you choose different ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose appropriate data weight vector (this will eventually become a diagonal matrix)\n",
    "\n",
    "# TODO: play around with this vector and see the results\n",
    "#----------\n",
    "\n",
    "wls_W1_vec = np.concatenate((np.ones(X.shape[0]), np.zeros(X.shape[0])))\n",
    "\n",
    "#----------\n",
    "\n",
    "w_wls, train_error_wls = WLS(X_hat_with_bad, y_with_bad, wls_W1_vec, verbose=True)\n",
    "test_error_wls = np.linalg.norm(X_hat @ w_wls - y_test)\n",
    "print(\"Testing error:\", test_error_wls)\n",
    "print(\"\\n\")\n",
    "\n",
    "w_wls = [[w_wls, \"WLS\"]]\n",
    "\n",
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_wls, y_obs_bad=y_obs_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO (optional): What $W_1$s did you try? How did they affect the regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tikhonov Regularization\n",
    "\n",
    "Tikhonov regularization is the combination of everything we've seen so far: ordinary least squares (OLS), giving weight to each sample via the $W_1$ matrix, and $\\ell_2$-norm penalization. However, we can also generalize the last penalty: instead of one $\\lambda$, use another another matrix $W_2$ that gives penalization weights to each element. For example, if we really cared about only allowing low-order polynomial terms, we would $\\textit{decrease}$ the weights on the first few $w_i$s, as we don't wish to penalize \"using\" those. Putting this all together, we get the following form:\n",
    "\n",
    "$$\\min_{w} \\|W_1(Xw - y)\\|_2^2+ \\|W_2(w - w_0)\\|_2^2$$\n",
    "$$W_1 = diag(w_1, w_2, \\dots), W_2 = diag(\\lambda_1, \\lambda_2, \\dots)$$\n",
    "\n",
    "One thing that may seem new is the $w - w_0$ term. To understand this, consider what we saw before: we just penalized the vector $w$, meaning that the longer $w$ was from the origin, the higher the penalty. However, we can generalize this as well: instead, we can penalize how far it is from some predetermined \"center\" $w_0$.\n",
    "\n",
    "Since this is the most robust method, we will juxtapose it with the performance of all methods seen so far on the entire good + bad dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Implement Tikhonov Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tikhonov(X, y, W1_vec, W2_vec, w_0, verbose=False):\n",
    "    \n",
    "    W1 = np.diag(W1_vec)\n",
    "    W2 = np.diag(W2_vec)\n",
    "    #----------\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_tilde = # Complete this\n",
    "    w = # Complete this to find optimal w\n",
    "    \n",
    "    #----------\n",
    "    error = np.linalg.norm(X @ w - y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"--------------------Tikhonov-----------------------\\n\")\n",
    "        print(\"The optimal w is\", np.ndarray.flatten(w))\n",
    "        print(\"\\n\")\n",
    "        print(\"Training error:\", error)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return w, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_bad, train_error_ls_bad = OLS(X_hat_with_bad, y_with_bad, verbose=True)\n",
    "test_error_ls_bad = np.linalg.norm(X_hat @ w_ls_bad - y_test)\n",
    "print(\"Testing error:\", test_error_ls_bad)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ridge\n",
    "ridge_lmbda = 10\n",
    "w_rr_bad, train_error_rr_bad = ridge(X_hat_with_bad, y_with_bad, ridge_lmbda, verbose=True)\n",
    "test_error_rr = np.linalg.norm(X_hat @ w_rr_bad - y_test)\n",
    "print(\"Testing error:\", test_error_rr)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# WLS: Only trust first half of data\n",
    "W1_vec = np.concatenate((np.ones(X.shape[0]), np.zeros(X.shape[0])))\n",
    "w_wls, train_error_wls = WLS(X_hat_with_bad, y_with_bad, W1_vec, verbose=True)\n",
    "test_error_wls = np.linalg.norm(X_hat @ w_wls - y_test)\n",
    "print(\"Testing error:\", test_error_wls)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Tikhonov:\n",
    "# Constant weights on all terms\n",
    "W2_vec = np.ones(X_hat.shape[1]) * 10\n",
    "\n",
    "# Penalize distance to origin\n",
    "w_0 = np.zeros(X_hat.shape[1])\n",
    "\n",
    "w_tikh, train_error_tikh = tikhonov(X_hat_with_bad, y_with_bad, W1_vec, W2_vec, w_0, verbose=True)\n",
    "test_error_tikh = np.linalg.norm(X_hat @ w_tikh - y_test)\n",
    "print(\"Testing error:\", test_error_tikh)\n",
    "print(\"\\n\")\n",
    "\n",
    "w_final = [[w_ls_bad, \"OLS\"], [w_rr_bad, \"Ridge\"], [w_wls, \"WLS\"], [w_tikh, \"Tikhonov\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_final, y_obs_bad=y_obs_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting Polynomial Features (optional)\n",
    "Using Tikhonov regularization, we have great control over how we want our regressor to end up. Don't like some of the data? Specify $W_1$ matrix to down weight such data. Interested in only allowing certain polynomial orders? Specify $W_2$ matrix to prefer such features. Below, you can see the result of designing the $W_2$ matrix to control the degree of our regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_control(highest_degree_desired):\n",
    "    # off-by-one due to existence of 0 degree terms\n",
    "    allowed = np.ones(highest_degree_desired + 1)\n",
    "    disallowed = np.ones(X_hat.shape[1] - (highest_degree_desired + 1)) * 10**15\n",
    "    return np.concatenate((allowed, disallowed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for highest_degree in np.arange(X_hat.shape[1]):\n",
    "    W2_vec = degree_control(highest_degree)\n",
    "    w_tikh, train_error_tikh = tikhonov(X_hat_with_bad, y_with_bad, W1_vec, W2_vec, w_0, verbose=True)\n",
    "    test_error_tikh = np.linalg.norm(X_hat @ w_tikh - y_test)\n",
    "    print(\"Testing error:\", test_error_tikh)\n",
    "    print(\"\\n\")\n",
    "    w_tikh_w2 = [[w_tikh, \"Degree \" + str(highest_degree)]]\n",
    "    plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_tikh_w2, y_obs_bad=y_obs_bad)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikhonov Playground (optional)\n",
    "\n",
    "Now that you have 3 hyperparameters, $W_1, W_2, \\vec{w}_0$, there is so much fun to be had! Try out different values and see how it affects your regressor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change these values to see what happens!\n",
    "\n",
    "W1_vec = np.concatenate((np.ones(X.shape[0]), np.zeros(X.shape[0])))\n",
    "\n",
    "W2_vec = np.ones(X_hat.shape[1]) * 10\n",
    "\n",
    "w_0 = np.zeros(X_hat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tikh, train_error_tikh = tikhonov(X_hat_with_bad, y_with_bad, W1_vec, W2_vec, w_0, verbose=True)\n",
    "test_error_tikh = np.linalg.norm(X_hat @ w_tikh - y_test)\n",
    "print(\"Testing error:\", test_error_tikh)\n",
    "print(\"\\n\")\n",
    "w_tikh = [[w_tikh, \"Tikhonov\"]]\n",
    "plot_result(X_hat, X, X_true, y_obs, y_true, y_test, w_tikh, y_obs_bad=y_obs_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO (optional): What hyperparmaters did you try? How did they affect the regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Things to Try (optional)\n",
    "\n",
    "Now that you've seen these methods in action, try and play around with different values. Some suggestions are to change data_fn() to generate a different set of data, or change the poly_degree argument in generate_data() to choose a different order polynomial regressor. It may be interesting to see what happens if you choose the data_fn() to be a lower order than the polynomial trying to fit it. There, overfitting will be on full display."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
