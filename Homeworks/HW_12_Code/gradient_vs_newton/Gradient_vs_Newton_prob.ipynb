{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent vs. Newton's Method\n",
    "\n",
    "In this notebook, we will analyze how first and second order descent methods (gradient descent and Newton's method respectively) act on different functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_plt_lim(x_array, func):\n",
    "    x_min, x_max = np.min(x_array[:,0]), np.max(x_array[:,0])\n",
    "    y_min, y_max = np.min(x_array[:,1]), np.max(x_array[:,1])\n",
    "    \n",
    "    x = np.linspace((x_min-0.1) *1.01, (x_max+0.1) *1.01)\n",
    "    y = np.linspace((y_min-0.1) *1.01, (y_max+0.1) *1.01)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = func([xx, yy])\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def visualize(x_array, func, watch_path=False):\n",
    "    x_array = np.array(x_array)\n",
    "    x, y, z = get_plt_lim(x_array, func)\n",
    "    \n",
    "    if watch_path:\n",
    "        for i in np.arange(len(x_array)):\n",
    "            plt.clf()\n",
    "            plt.scatter(x_array[:i,0], x_array[:i,1], s = 5, c=\"red\")\n",
    "            plt.plot(x_array[:i,0],x_array[:i,1], 'o-', zorder=2, c=\"red\")\n",
    "            plt.contourf(x, y, z)\n",
    "            plt.colorbar()\n",
    "            plt.xlabel('x_1')\n",
    "            plt.ylabel('x_2')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "    else:\n",
    "        plt.scatter(x_array[:,0], x_array[:,1], s = 5, c=\"red\")\n",
    "        plt.plot(x_array[:,0],x_array[:,1], 'o-', zorder=2, c=\"red\")\n",
    "        plt.contourf(x, y, z)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('x_1')\n",
    "        plt.ylabel('x_2')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_comparison(x_grad, x_newton, func, watch_path=False):\n",
    "    \n",
    "    x_grad = np.array(x_grad)\n",
    "    grad_x, grad_y, grad_z = get_plt_lim(x_grad, func)\n",
    "    \n",
    "    x_newton = np.array(x_newton)\n",
    "    newton_x, newton_y, newton_z = get_plt_lim(x_grad, func)\n",
    "    \n",
    "    if watch_path:\n",
    "        for i in np.arange(np.max([len(x_grad), len(x_newton)])):\n",
    "            plt.clf()\n",
    "                           \n",
    "            plt.scatter(x_grad[:i,0], x_grad[:i,1], s = 5, c=\"red\")\n",
    "            plt.plot(x_grad[:i,0],x_grad[:i,1], 'o-', zorder=2,  label = 'Gradient Descent', c=\"red\")   \n",
    "                           \n",
    "            plt.scatter(x_newton[:i,0], x_newton[:i,1], s = 5, c=\"orange\")\n",
    "            plt.plot(x_newton[:i,0],x_newton[:i,1], 'o-', zorder=2,  label = \"Newton's method\", c=\"orange\") \n",
    "                   \n",
    "            plt.contourf(grad_x, grad_y, grad_z)\n",
    "            plt.colorbar()\n",
    "            plt.xlabel('x_1')\n",
    "            plt.ylabel('x_2')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "    else:\n",
    "    \n",
    "        plt.scatter(x_grad[:,0], x_grad[:,1], s = 5, c=\"red\")\n",
    "        plt.plot(x_grad[:,0],x_grad[:,1], 'o-', zorder=2,  label = 'Gradient Descent', c=\"red\")   \n",
    "\n",
    "        plt.scatter(x_newton[:,0], x_newton[:,1], s = 5, c=\"orange\")\n",
    "        plt.plot(x_newton[:,0],x_newton[:,1], 'o-', zorder=2,  label = \"Newton's method\", c=\"orange\") \n",
    "    \n",
    "        plt.contourf(grad_x, grad_y, grad_z)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('x_1')\n",
    "        plt.ylabel('x_2')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Implementing Descent Methods\n",
    "\n",
    "In this part, you will need to implement gradient descent and Newton's method. The appropriate parameters are given for each function, and it is your responsibility to return the \"path\" that it takes via a list of $[x_0, x_1, \\dots, x_k]$, in addition to the number of iterations it took to either converge or not\n",
    ". \n",
    "\n",
    "Note that, due to constraints imposed by working with floats, it is rare that we will perfectly converge (i.e. reach an $x^*$ such that it doesn't change). Because of this, we introduce a parameter $\\epsilon$ that will act as a threshold. If your next guess isn't too far from your previous guess, you can assume that we have converged. The smaller $\\epsilon$ is, the more accurate our guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "# Gradient descent (first order method)\n",
    "def gradient_descent(gradient, x, stepsize, eps = 1e-6, max_iters = 100):\n",
    "    \n",
    "    x_array = [x]\n",
    "    num_iters =1\n",
    "    # Your code here\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    return x_array, num_iters\n",
    "\n",
    "# Netwon's method (second order method)\n",
    "def Newton(gradient, Hessian, x,  stepsize=1.0, eps = 1e-6, max_iters = 100):\n",
    "    # Your code here\n",
    "    x_array = [x]\n",
    "    num_iters = 1\n",
    "    # Your code here\n",
    "    #####\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    return x_array, num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Minimizing a quadratic\n",
    "Consider the following unconstrained convex optimization problem,\n",
    "\\begin{aligned}\n",
    "\\min_{x_1,x_2 \\in \\mathbb{R}} f(x_1,x_2) =  \\frac{1}{2} \\left (32x_1^2 + x_2^2 \\right)\n",
    "\\end{aligned}\n",
    "\n",
    "Graphically, this is a parabaloid which grows much faster in the $x\\; (x_1)$ direction than the $y\\; (x_2)$ direction. Clearly, the optimal value of the problem is $0$ and $x_1^* = x_2^*= 0$.\n",
    "\n",
    "## 1a. Using gradient descent\n",
    "\n",
    "For this part of the question, it is useful to remember a fact proved in Problem 4 of Homework 6. Specifically, the condition for convergence.\n",
    "\n",
    "### TODO: Complete the function below to return the gradient computed at $x$ for the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    '''Return gradient at x'''\n",
    "    # TODO: Your code here\n",
    "    return\n",
    "\n",
    "def f(x):\n",
    "    '''Return f(x)'''\n",
    "    fx = 16*x[0]**2 + 0.5*x[1]**2\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in action\n",
    "Suppose you start with $x_0 = \\begin{bmatrix} 0.1 \\\\ 1\\end{bmatrix}$. \n",
    "\n",
    "### TODO: Run gradient descent for the following stepsizes and compare the paths traced by $x_k$:\n",
    "1. $\\eta = \\frac{2}{31.9}$\n",
    "2. $\\eta = \\frac{2}{35}$\n",
    "3. $\\eta = \\frac{2}{128}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 2/31.9\n",
    "x0 = np.array([0.1,1])\n",
    "\n",
    "x_array, num_iters = gradient_descent(gradient, x = x0, stepsize=eta, max_iters=1000)\n",
    "\n",
    "# If you want to watch the descent live, set watch_path=True\n",
    "# WARNING: For a descent that diverges, this will take REALLY long.\n",
    "visualize(x_array, f, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? Why or why not? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 2/35\n",
    "\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters = gradient_descent(gradient, x = x0, stepsize=eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? Why or why not? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta= 2/128\n",
    "\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters = gradient_descent(gradient, x = x0, stepsize=eta, max_iters=1000)\n",
    "x_grad1 = x_array\n",
    "\n",
    "visualize(x_array, f, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? Why or why not? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Using Newton's method\n",
    "Next we will use Newton's method to see if convergence is faster. \n",
    "### TODO: Complete the function below to return the Hessian computed at $x$ for the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hessian(x):\n",
    "    '''Return Hessian at x'''\n",
    "    # TODO: Your code here\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method in action\n",
    "Suppose you start with $x_0 = \\begin{bmatrix}0.1 \\\\ 1\\end{bmatrix}$. \n",
    "### TODO: Run Newton's method for the following stepsizes and compare the paths traced by $x_k$:\n",
    "1. $\\eta = 2.2$\n",
    "2. $\\eta =1$\n",
    "3. $\\eta = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 2.2\n",
    "\n",
    "x0 = np.array([0.1,1])\n",
    "x_array, num_iters = Newton(gradient, Hessian, x = x0, stepsize = eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.1,1])\n",
    "eta = 1.0\n",
    "x_array, num_iters = Newton(gradient, Hessian, x = x0, stepsize = eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.1,1])\n",
    "eta = 0.5\n",
    "x_array, num_iters = Newton(gradient, Hessian, x = x0, stepsize = eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f, watch_path=False)\n",
    "x_newton1 = x_array\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Compare the paths taken by gradient descent with stepsize $\\frac{2}{128}$ and Newton's method with stepsize $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "visualize_comparison(x_grad1, x_newton1, f, watch_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Compare the methods. How did they differ in their descent paths? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimizing a non-quadratic objective\n",
    "Next we consider a problem that involves minimization of an objective function that is not quadratic,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\min_{x_1,x_2 \\in \\mathbb{R}} f(x_1,x_2) =  \\frac{1}{2} \\left (10x_1^2 + x_2^2 \\right) +  5\\log(1 + e^{-x_1 -x_2})\n",
    "\\end{aligned}\n",
    "\n",
    "### TODO: Complete the functions below to return the gradient and Hessian computed at $x$ for the problem above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return 0.5*(10*x[0]**2 + x[1]**2) + 5 * np.log(1 + np.exp(-x[0] - x[1]))\n",
    "\n",
    "def gradient2(x):\n",
    "    '''Return gradient at x'''\n",
    "    # TODO: Your code here\n",
    "    return\n",
    "\n",
    "def Hessian2(x):\n",
    "    '''Return Hessian at x'''\n",
    "    # TODO: Your code here\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Gradient Descent in action\n",
    "Suppose you start with $x_0 = \\begin{bmatrix}-20 \\\\ -20\\end{bmatrix}$. \n",
    "\n",
    "### TODO: Run gradient descent using stepsize of $\\frac{1}{8}$ and plot the trajectory as well as optimal value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta= 1/8\n",
    "x0 = np.array([-20,-20])\n",
    "x_array, num_iters = gradient_descent(gradient2, x = x0, stepsize=eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f2, watch_path=False)\n",
    "\n",
    "x_grad2 = x_array\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f2(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Newton's method in action\n",
    "Suppose you start with $x_0 = \\begin{bmatrix}20 \\\\ 20\\end{bmatrix}$. \n",
    "\n",
    "### TODO: Run Newton's method using stepsize of $1$ and $\\frac{1}{4}$ and plot the trajectory as well as optimal value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-20,-20])\n",
    "eta = 1.0\n",
    "x_array, num_iters = Newton(gradient2, Hessian2, x = x0, stepsize = eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f2, watch_path=False)\n",
    "\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f2(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-20,-20])\n",
    "eta = 1.0/4.0\n",
    "x_array, num_iters = Newton(gradient2, Hessian2, x = x0, stepsize = eta, max_iters=1000)\n",
    "\n",
    "visualize(x_array, f2, watch_path=False)\n",
    "x_newton2 = x_array\n",
    "print(\"Stepsize = \" + str(eta) + \", Num iterations \" + str(num_iters) +  \", Final x: \" +  str(x_array[-1]) + \", Final objective value: \" + str(f2(x_array[-1]).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Did it converge? If so, how many steps it take? How fast did it converge in the $x_1$ and $x_2$ directions, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Compare the paths taken by gradient descent with stepsize 1/8 and Newton's method with stepsize 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_comparison(x_grad2, x_newton2, f2, watch_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Compare the methods. How did they differ in their descent paths? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with stepsizes for gradient descent and Newton's method in the cells above and see when things start to diverge and how the paths taken evolve as stepsize changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit: \n",
    "Vignesh Subramanian, Spring 2019\n",
    "\n",
    "Sean Farhat, Spring 2020\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
