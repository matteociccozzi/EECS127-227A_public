{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Vs. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we compare using two methods for classification: Ridge Regression, which we've seen over and over gain in this course, and a soft-margin Support Vector Machine, which was the de-facto method for classifcation for a large chunk of machine learning's history. As we shall see, formulating the problem as different optimization problems (here SVM and Ridge Regression) makes a difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualization.\n",
    "\n",
    "def plot_boundry(X, y, fitted_model):\n",
    "    \n",
    "    # Usage: plot_boundry(X, y, fitted_model)\n",
    "    #     X: your features, where each row is a data sample\n",
    "    #     y: your labels, can be 0/1 or -1/1\n",
    "    #     fitted_model: a scipy TRAINED model, such as sklearn.svm.SVC\n",
    "    \n",
    "    plt.figure(figsize=(9.8,5), dpi=100)\n",
    "    \n",
    "    for i, plot_type in enumerate(['Decision Boundary']):\n",
    "        plt.subplot(1,2,i+1)\n",
    "\n",
    "        mesh_step_size = 0.5  # step size in the mesh\n",
    "        x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "        y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "        x_max = 110\n",
    "        y_max = 60\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size))\n",
    "        if i == 0:\n",
    "            Z = fitted_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = np.sign(Z)\n",
    "        else:\n",
    "            try:\n",
    "                Z = fitted_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "            except:\n",
    "                plt.text(0.4, 0.5, 'Probabilities Unavailable', horizontalalignment='center',\n",
    "                     verticalalignment='center', transform = plt.gca().transAxes, fontsize=12)\n",
    "                plt.axis('off')\n",
    "                break\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.scatter(X[y==0,0], X[y==0,1], alpha=0.4, label='Edible', s=5)\n",
    "        plt.scatter(X[y==1,0], X[y==1,1], alpha=0.4, label='Posionous', s=5)\n",
    "        plt.imshow(Z, interpolation='nearest', cmap='RdYlBu_r', alpha=0.15, \n",
    "                   extent=(x_min, x_max, y_min, y_max), origin='lower')\n",
    "        plt.title(plot_type)\n",
    "        plt.gca().set_aspect('equal');\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.08, wspace=0.02)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = np.load(\"ridge_vs_svm_data_train.npy\")\n",
    "X_train = train_data[:, 1:]\n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "test_data= np.load(\"ridge_vs_svm_data_train.npy\")\n",
    "X_test = test_data[:, 1:]\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the training data to get a sense of the distribution. Note the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], alpha=0.4, s=5)\n",
    "plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], alpha=0.4, s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Recall that the soft-margin SVM as defined as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{w \\in \\mathbb{R}^m ,\\: b \\in \\mathbb{R},\\: \\zeta_i \\in \\mathbb{R}^n} \\: \\: & \\frac{1}{2} \\|w\\|_2^2 + C \\sum_{i=1}^n \\zeta_i \\\\\n",
    "\\text{s.t. } & 1 - \\zeta_i - y_i(x_i^\\top w  - b) \\leq 0 \\\\\n",
    "& \\zeta_i \\geq 0\n",
    "\\end{align}$$\n",
    "\n",
    "where $x_i$ is the $i$th data point, $y_i \\in \\{-1, 1\\}$ is the label, and $C$ is a hyperparameter that controls how \"soft\" the margin is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Fill in the code below to run a __linear__ SVM to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = None # your trained model\n",
    "y_pred = None # the prediction of your trained model on the testing data\n",
    "\n",
    "########## Your beautiful code starts here ##########\n",
    "\n",
    "# TODO: Write code to train an SVM, and generate prediction y_pred.\n",
    "# The documentation for sklearn's SVM: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# Optional: Try using different kernels.  How does the value of C matter? \n",
    "\n",
    "########## Your beautiful code ends here ##########\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(\"Test Accuracy: {}\".format(accuracy)) \n",
    "\n",
    "plot_boundry(X_test, y_test, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Fill in the code below to run ridge regression to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = None # your trained model (as trained by scipy)\n",
    "y_pred_sign = None # the prediction of your trained model on the testing data\n",
    "\n",
    "# convert the labels from 0 and 1 to -1 and 1\n",
    "y_train_sign = np.array(y_train)\n",
    "y_test_sign = np.array(y_test)\n",
    "y_train_sign[y_train_sign == 0] = -1\n",
    "y_test_sign[y_test_sign == 0] = -1\n",
    "\n",
    "# for the regularization parameter lambda, you can try something around 0.1 :)\n",
    "# Optional: try choosing different parameters\n",
    "llambda = 0.1\n",
    "\n",
    "########## Your beautiful code starts here ##########\n",
    "\n",
    "# TODO: train a fitted_model and run prediction to generate y_pred_sign\n",
    "\n",
    "########## Your beautiful code ends here ##########\n",
    "\n",
    "accuracy = accuracy_score(y_pred_sign, y_test_sign)\n",
    "print(\"Test Accuracy: {}\".format(accuracy))\n",
    "\n",
    "plot_boundry(X_test, y_test, fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do We See SVM Outperforming Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we saw that SVM outperforms ridge regression. The data was actually synthetically generated from two Gaussians --- but remember the two outliers? Can you see how they are impacting the classifer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Comment on your observations for both classifiers. How well did each classify the points? How did they react to the outliers? If you tried different kernels for the SVM, how did they behave?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the data was produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try changing the positions of the outliers to see how they impact the performanace\n",
    "\n",
    "n = 100\n",
    "cov = np.eye(2) * 20\n",
    "\n",
    "pos = np.hstack([\n",
    "    np.ones(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([5, 5], cov, size=n),\n",
    "])\n",
    "neg = np.hstack([\n",
    "    np.zeros(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([-5, -5], cov, size=n),\n",
    "])\n",
    "\n",
    "syn = np.vstack([pos, neg])\n",
    "\n",
    "outliers = np.array([\n",
    "    [0, 80, 50,],\n",
    "    [0, 100, 50,],\n",
    "])\n",
    "\n",
    "syn = np.vstack([pos, neg, outliers])\n",
    "np.random.shuffle(syn)\n",
    "np.save(\"ridge_vs_svm_data_train.npy\", syn)\n",
    "\n",
    "\n",
    "pos_test = np.hstack([\n",
    "    np.ones(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([5, 5], cov, size=n),\n",
    "])\n",
    "neg_test = np.hstack([\n",
    "    np.zeros(n).reshape([-1, 1]),\n",
    "    np.random.multivariate_normal([-5, -5], cov, size=n),\n",
    "])\n",
    "\n",
    "syn = np.vstack([pos_test, neg_test])\n",
    "\n",
    "np.random.shuffle(syn)\n",
    "np.save(\"ridge_vs_svm_data_test.npy\", syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring 2019: Mong H. Ng, Prof. Ranade <br>\n",
    "Plotting function from https://github.com/devssh/svm/blob/master/SVM%20Python/Classifier%20Visualization.ipynb\n",
    "\n",
    "Spring 2020: Sean Farhat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
